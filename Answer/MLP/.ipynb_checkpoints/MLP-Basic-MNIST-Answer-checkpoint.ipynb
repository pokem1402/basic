{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 데이터 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from six.moves import urllib\n",
    "import gzip\n",
    "import cPickle\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "SOURCE_URL = 'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "FILENAME = SOURCE_URL.split('/')[-1]\n",
    "DATA_DIR = '../datasets'\n",
    "\n",
    "def maybe_download(data_dir):\n",
    "    filepath = os.path.join(data_dir, FILENAME)\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    if not os.path.isfile(filepath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\r>> Downloading {} {:.1f} %'.format(\n",
    "                FILENAME, float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "        filepath, _ = urllib.request.urlretrieve(SOURCE_URL, filepath, _progress)\n",
    "        print()\n",
    "        statinfo = os.stat(filepath)\n",
    "        print('Successfully donloaded', FILENAME, statinfo.st_size, 'bytes.')\n",
    "\n",
    "def load(data_dir, subset='train'):\n",
    "    maybe_download(data_dir)\n",
    "    filepath = os.path.join(data_dir, FILENAME)\n",
    "    \n",
    "    f = gzip.open(filepath, 'rb')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    if subset == 'train':\n",
    "        trainx, trainy = train_set\n",
    "        trainx = trainx.astype(np.float32).reshape(trainx.shape[0], 28, 28)\n",
    "        trainy = trainy.astype(np.uint8)\n",
    "        return trainx, trainy\n",
    "    elif subset == 'test':\n",
    "        testx, testy = test_set\n",
    "        testx = testx.astype(np.float32).reshape(testx.shape[0], 28, 28)\n",
    "        testy = testy.astype(np.uint8)\n",
    "        return testx, testy\n",
    "    elif subset== 'valid':\n",
    "        validx, validy = valid_set\n",
    "        validx = validx.astype(np.float32).reshape(validx.shape[0], 28, 28)\n",
    "        validy = validy.astype(np.uint8)\n",
    "        return validx, validy\n",
    "    else:\n",
    "        raise NotImplementedError('subset should be train or valid or test')\n",
    "\n",
    "# Load data\n",
    "train_data, train_label = load(DATA_DIR, 'train')\n",
    "valid_data, valid_label = load(DATA_DIR, 'valid')\n",
    "test_data, test_label = load(DATA_DIR, 'test')\n",
    "\n",
    "# concatenate train and valid data as train data\n",
    "train_data = np.concatenate((train_data, valid_data))\n",
    "train_label = np.concatenate((train_label, valid_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 데이터 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# size of MNIST\n",
    "print train_data.shape\n",
    "print train_label.shape\n",
    "print test_data.shape\n",
    "print test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD8CAYAAACINTRsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEOoAkJD/A725v0z+kMq3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4cc809f2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show data\n",
    "_, (ax1, ax2) = plt.subplots(1, 2)\n",
    "sample_data = train_data[0]\n",
    "ax1.imshow(sample_data, cmap=plt.cm.Greys);\n",
    "ax2.hist(sample_data, bins=20, range=[0, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesImage(54,36;334.8x217.44)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRfiBoAg/EBThB4Ii/EBQhB8IivADQf0/sEWOix6VKakAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4cf845d950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(plt.imshow(sample_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing (데이터 전처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "SUMMARY_DIR = './summary'\n",
    "TRAIN_DIR = SUMMARY_DIR + '/train'\n",
    "TEST_DIR = SUMMARY_DIR + '/test'\n",
    "\n",
    "if os.path.exists(SUMMARY_DIR):\n",
    "    shutil.rmtree(SUMMARY_DIR)\n",
    "if not os.path.exists(SUMMARY_DIR):\n",
    "    os.makedirs(SUMMARY_DIR)\n",
    "    os.makedirs(TRAIN_DIR)\n",
    "    os.makedirs(TEST_DIR)\n",
    "\n",
    "def get_next_batch(data, label, batch_size):\n",
    "    \"\"\"\n",
    "    get 'batch_size' amount of data and label randomly\n",
    "\n",
    "    Args:\n",
    "        data: data\n",
    "        label: label\n",
    "        batch_size: # of data to get\n",
    "\n",
    "    Returns:\n",
    "        batch_data: data of 'batch_size'\n",
    "        batch_label: coresponding label of batch_data\n",
    "    \"\"\"\n",
    "    n_data = data.shape[0]\n",
    "    random_idx = random.sample(range(1, n_data), batch_size)\n",
    "    \n",
    "    batch_data = data[random_idx]\n",
    "    batch_label = label[random_idx]\n",
    "    return batch_data, batch_label\n",
    "\n",
    "def get_one_hot(label):\n",
    "    \"\"\"\n",
    "    get one hot encoded label matrix\n",
    "\n",
    "    Args:\n",
    "        label: original label\n",
    "\n",
    "    Returns:\n",
    "        One hot encoded label matrix\n",
    "    \"\"\"\n",
    "    # Make 0 initialized numpy array with shape of [label.shape[0], 10]\n",
    "    one_hot = np.zeros((label.shape[0], 10))\n",
    "    # Fill up the array according to the input label\n",
    "    one_hot[np.arange(label.shape[0]), label.astype(int)] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation (1)\n",
    "\n",
    "\n",
    "## Loss function (손실 함수) : Cross Entropy\n",
    "\n",
    "# <center> \\\\( L(y_i, f(x_i; W)) = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{k=1} y_{i,j} log(f(x_i)_k)\\\\)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_entropy_loss(y_true, y_hat, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    compute cross entropy\n",
    "\n",
    "    Args:\n",
    "        y_true: true label\n",
    "        y_hat: predicted label\n",
    "        epsilon: small value to prevent NaN in log\n",
    "\n",
    "    Returns:\n",
    "        cross entropy loss\n",
    "    Hints\n",
    "        - tf.reduce_mean  \n",
    "        - tf.reduce_sum\n",
    "        - tf.log\n",
    "    \"\"\"\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(y_hat + epsilon), axis=[1]), name='cross_entropy')\n",
    "    return cross_entropy\n",
    "\n",
    "def get_accuracy(y_true, y_hat):\n",
    "    \"\"\"\n",
    "    accuracy 구하기\n",
    "\n",
    "    Args:\n",
    "        y_true: true label\n",
    "        y_hat: predicted label\n",
    "    Returns:\n",
    "        Accuracy\n",
    "    Hints\n",
    "        - tf.equal : 값을 비교하여 동일하면 1, 다르면 0을 return\n",
    "        - tf.argmax : 값이 최대인 위치의 index를 return\n",
    "        - tf.cast : data type을 변경하는 함수\n",
    "        - tf.reduce_mean : 지정한 축(axis)에 대해 평균을 취하는 함수.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('accuracy'):\n",
    "        # Compare the highest indices between the predicted label and the true label\n",
    "        correct_prediction = tf.equal(tf.argmax(y_hat, 1), tf.argmax(y_true, 1), name='correct_prediction')\n",
    "        # Compute accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter (하이퍼 파라미터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hypyerparameters\n",
    "learning_rate = 0.1\n",
    "max_iter = 5000\n",
    "step_size = max_iter/10\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation (2)\n",
    "## Linear Classifier (선형 분류기)\n",
    "\n",
    "## <center> \\\\( f(x) = xW+b \\\\)</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(inputs, out_dim, name='linear'):\n",
    "    \"\"\"\n",
    "    Args :\n",
    "        Inputs : Input tensor\n",
    "        out_dim : output dimension\n",
    "        name : name for parameter\n",
    "    Returns:\n",
    "        output\n",
    "    Hints\n",
    "        - tf.Variable  \n",
    "        - tf.matmul\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        # Get dimension of inputs (not batch)\n",
    "        in_dim = inputs.get_shape().as_list()[1]\n",
    "        # initializer for weight matrix -> tf.zeros([])\n",
    "        W = tf.Variable(tf.random_normal([in_dim, out_dim]), name='weights')\n",
    "        # initializer for bias vector -> tf.zeros([])\n",
    "        b = tf.Variable(tf.zeros([out_dim]), name='biases')\n",
    "        # compute logits\n",
    "        y_logits = tf.matmul(inputs, W) + b\n",
    "    return y_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. With Softmax Linear Classifier\n",
    "## <center> \\\\( f(x_i; W,b) =  \\sigma(xW+b) \\\\)</center>\n",
    "\n",
    "> where $\\sigma$ is softmax function, i.e.,\n",
    "\n",
    "## <center> \\\\( \\sigma(x)_i =  \\frac{e^{x_i}}{\\sum_j e^{x_j}}  \\\\)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(x):\n",
    "    \"\"\"\n",
    "        Softmax Linear Classifier Model\n",
    "        \n",
    "        Args:\n",
    "            x : input\n",
    "            k : the number of classes, i.e., output dimension of the model\n",
    "        Returns:\n",
    "            output\n",
    "            weights list\n",
    "            bias list\n",
    "        Hints:\n",
    "            tf.nn.softmax : softmax function\n",
    "    \"\"\"\n",
    "    out = linear(x, 10)\n",
    "    out = tf.nn.softmax(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500th iteration, loss: 1.0986, test accuracy: 0.7516\n",
      "1000th iteration, loss: 1.2891, test accuracy: 0.8105\n",
      "1500th iteration, loss: 0.8784, test accuracy: 0.8335\n",
      "2000th iteration, loss: 0.6120, test accuracy: 0.8477\n",
      "2500th iteration, loss: 0.4688, test accuracy: 0.8558\n",
      "3000th iteration, loss: 0.8337, test accuracy: 0.8628\n",
      "3500th iteration, loss: 0.8980, test accuracy: 0.8675\n",
      "4000th iteration, loss: 0.5199, test accuracy: 0.8713\n",
      "4500th iteration, loss: 0.5327, test accuracy: 0.8736\n",
      "5000th iteration, loss: 0.3674, test accuracy: 0.8772\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train_label_hot = get_one_hot(train_label)\n",
    "test_label_hot = get_one_hot(test_label)\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 0) Data_flatten\n",
    "# 3차원 Tensor를 2차원 maxtrix로 reshape\n",
    "# Hint : np.reshape()\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "train_data = train_data.reshape(-1, 784)\n",
    "test_data = test_data.reshape(-1, 784)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 1) Declare Placeholders\n",
    "# input, true_label을 위한 placeholder 선언\n",
    "# Hint : tf.placeholder, input과 ouput이 어떤 차원을 가져야할지 고민해봅시다.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='input')\n",
    "y_true = tf.placeholder(tf.float32, [None, 10], name='y_true')\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 2) model declare\n",
    "# Prediction을 위해 기존의 정의한 model을 사용합니다.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "y_pred = model_1(x)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 3) obtain loss & accuracy\n",
    "# 앞서 정의한 function들을 통해 loss와 accuracy를 얻습니다.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "cross_entropy = get_cross_entropy_loss(y_true, y_pred)\n",
    "accuracy = get_accuracy(y_true, y_pred)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 4) Optimizer\n",
    "# Parameter Update를 위한 Optimizer를 선언합니다. Learning rate는 learning_rate로 선언된 변수를 사용합니다.\n",
    "# Hint : tf.train.GradientDescentOptimizer\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 7) Session \n",
    "# tensorflow의 연산을 수행하기 위해 session을 호출합니다.\n",
    "# 8) variable iniializer \n",
    "# tensorflow의 variable들을 초기화 하기 위해 initializer를 사용합니다.\n",
    "# Hint : tf.global_variables_initializer\n",
    "# 9) training \n",
    "# 학습에서 사용할 iteration 수는 위의 iteration을 사용합니다. Loss 및 accuracy를 check할 때 step_size를 사용합니다.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "  \n",
    "    # Training\n",
    "    for step in range(max_iter):\n",
    "        # Get batch data and label\n",
    "        batch_x, batch_y = get_next_batch(train_data, train_label_hot, batch_size)\n",
    "        # train the network and calculate cross entropy\n",
    "        _, loss = sess.run([train_step, cross_entropy], feed_dict={x: batch_x, y_true: batch_y})\n",
    "        # calcualte accuracy\n",
    "        acc = sess.run(accuracy, feed_dict={x: test_data, y_true: test_label_hot})\n",
    "        # print loss (cross entropy) and accuracy at every 10th step\n",
    "        if (step + 1) % step_size == 0:\n",
    "            print(\"{}th iteration, loss: {:.4f}, test accuracy: {:.4f}\".format(step + 1, loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation (4)\n",
    "\n",
    "## Non-Linear Classifier & Multi-Layer Perceptron\n",
    "\n",
    "## <center> \\\\( f(x) = W^T_2\\sigma(W^T_1x+b_1)+b_2 \\\\)</center>\n",
    "\n",
    "### 위에서 완성한 linear function과 relu activation function을 활용하여, 1개의 hidden layer가 있는 MLP를 구현하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2(Inputs, hidden, out_dim):\n",
    "    \"\"\"\n",
    "        Neural Network with 1 Hidden layer.\n",
    "        Args:\n",
    "            x : input\n",
    "            k : the number of classes, i.e., output dimension of the model\n",
    "        Returns:\n",
    "            output\n",
    "            weights list\n",
    "            bias list\n",
    "        Hint:\n",
    "            tf.nn.relu : activation function\n",
    "            tf.nn.softmax : softmax function\n",
    "    \"\"\"\n",
    "    # First fully connected layer\n",
    "    h = linear(Inputs, hidden, 'h1')\n",
    "    # add non-linearity (using tf.nn.relu(input))\n",
    "    h_relu = tf.nn.relu(h)\n",
    "    # Second fully connected layer\n",
    "    y_logits = linear(h_relu, out_dim, 'out')\n",
    "    out = tf.nn.softmax(y_logits)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation (5)\n",
    "\n",
    "## Model Setting\n",
    "\n",
    "### 1. Training data 및 Test data의 각각의 image를 한 vector로 만들어서 train_data, test_data에 각각 저장하세요.\n",
    "#### Hint) 데이터 차원.\n",
    "### 2. Dataset로부터 받은 데이터(Image, label)를 담을 변수를 각각 x 및 y_true에 선언하고, DropOut을 위한 확률 값을 담을 변수를 keep_prob에 저장하세요.\n",
    "#### Hint) tf.placeholder\n",
    "### 3. Implementation (4)에서 구현한  Multilayer perceptron 함수와 softmax 함수를 통한 prediction 값을 y_hat에 저장하세요.\n",
    "#### Hint) tf.nn.softmax\n",
    "### 4. 3으로부터 얻은 결과를 통해 Implementation (1)에서 구현한 loss function을 통해 얻은 loss를 cross_entropy에 저장하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500th iteration, loss: 10.0718, test accuracy: 0.3901\n",
      "1000th iteration, loss: 5.8530, test accuracy: 0.5685\n",
      "1500th iteration, loss: 7.0956, test accuracy: 0.6353\n",
      "2000th iteration, loss: 5.3554, test accuracy: 0.7212\n",
      "2500th iteration, loss: 4.4373, test accuracy: 0.7320\n",
      "3000th iteration, loss: 4.8883, test accuracy: 0.7441\n",
      "3500th iteration, loss: 3.8416, test accuracy: 0.7446\n",
      "4000th iteration, loss: 4.5770, test accuracy: 0.7454\n",
      "4500th iteration, loss: 4.3343, test accuracy: 0.7488\n",
      "5000th iteration, loss: 4.6667, test accuracy: 0.7489\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train_label_hot = get_one_hot(train_label)\n",
    "test_label_hot = get_one_hot(test_label)\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 0) Data_flatten\n",
    "# 3차원 Tensor를 2차원 maxtrix로 reshape\n",
    "# Hint : np.reshape()\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "train_data = train_data.reshape(-1, 784)\n",
    "test_data = test_data.reshape(-1, 784)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 1) Declare Placeholders\n",
    "# input, true_label을 위한 placeholder 선언\n",
    "# Hint : tf.placeholder, input과 ouput이 어떤 차원을 가져야할지 고민해봅시다.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='input')\n",
    "y_true = tf.placeholder(tf.float32, [None, 10], name='y_true')\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 2) model declare\n",
    "# Prediction을 위해 기존의 정의한 model을 사용합니다.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "y_pred = model_2(x, 128, 10)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 3) obtain loss & accuracy\n",
    "# 앞서 정의한 function들을 통해 loss와 accuracy를 얻습니다.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "cross_entropy = get_cross_entropy_loss(y_true, y_pred)\n",
    "accuracy = get_accuracy(y_true, y_pred)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 4) Optimizer\n",
    "# Parameter Update를 위한 Optimizer를 선언합니다. Learning rate는 learning_rate로 선언된 변수를 사용합니다.\n",
    "# Hint : tf.train.GradientDescentOptimizer\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 7) Session \n",
    "# tensorflow의 연산을 수행하기 위해 session을 호출합니다.\n",
    "# 8) variable iniializer \n",
    "# tensorflow의 variable들을 초기화 하기 위해 initializer를 사용합니다.\n",
    "# Hint : tf.global_variables_initializer\n",
    "# 9) training \n",
    "# 학습에서 사용할 iteration 수는 위의 iteration을 사용합니다. Loss 및 accuracy를 check할 때 step_size를 사용합니다.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "  \n",
    "    # Training\n",
    "    for step in range(max_iter):\n",
    "        # Get batch data and label\n",
    "        batch_x, batch_y = get_next_batch(train_data, train_label_hot, batch_size)\n",
    "        # train the network and calculate cross entropy\n",
    "        _, loss = sess.run([train_step, cross_entropy], feed_dict={x: batch_x, y_true: batch_y})\n",
    "        # calcualte accuracy\n",
    "        acc = sess.run(accuracy, feed_dict={x: test_data, y_true: test_label_hot})\n",
    "        # print loss (cross entropy) and accuracy at every 10th step\n",
    "        if (step + 1) % step_size == 0:\n",
    "            print(\"{}th iteration, loss: {:.4f}, test accuracy: {:.4f}\".format(step + 1, loss, acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
