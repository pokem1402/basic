{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice with two_moon data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "## ffmpeg image to video.\n",
    "#ffmpeg -framerate 10 -i %4d_result.png -vcodec libx264 -pix_fmt yuv420p movie.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "%matplotlib inline\n",
    "\n",
    "N = 100\n",
    "K = 2\n",
    "\n",
    "def one_hot_coding(y):\n",
    "    k,n = np.max(y)+1, np.shape(y)[0]\n",
    "    one_hot_y = np.zeros([n,k])\n",
    "    one_hot_y[np.arange(n),y] = 1\n",
    "    return one_hot_y\n",
    "\n",
    "def class_number(y):\n",
    "    n = np.shape(y)[0]\n",
    "    y_number = np.argmax(y, axis=1) \n",
    "    return y_number\n",
    "\n",
    "def shape_scatter(X, y, weights, biases):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    assert len(weights) == len(biases), \"error\"\n",
    "    \n",
    "    for i,w in enumerate(weights):\n",
    "        if i==0:\n",
    "            Z = np.dot(np.c_[xx.ravel(), yy.ravel()], weights[i])+biases[i]\n",
    "        else:\n",
    "            Z = np.dot(np.maximum(0, Z), weights[i])+biases[i]\n",
    "    \n",
    "    Z = np.argmax(Z, axis=1)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig = plt.figure()\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40)\n",
    "    plt.show()\n",
    "def shape_save(X, y, weights, biases, iteration):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    assert len(weights) == len(biases), \"error\"\n",
    "    \n",
    "    for i,w in enumerate(weights):\n",
    "        if i==0:\n",
    "            Z = np.dot(np.c_[xx.ravel(), yy.ravel()], weights[i])+biases[i]\n",
    "        else:\n",
    "            Z = np.dot(np.maximum(0, Z), weights[i])+biases[i]\n",
    "    \n",
    "    Z = np.argmax(Z, axis=1)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig = plt.figure()\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40)\n",
    "    fig.savefig('./picture/'+str(format(iteration, '04'))+'_result.png')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    print(iteration, 'th saved.')\n",
    "    \n",
    "def scatter(X, y):\n",
    "    if len(y.shape) > 1: # one-hot coding\n",
    "        y = class_number(y)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "    plt.show()\n",
    "\n",
    "# # lets visualize the data:    \n",
    "X, y = make_moons(n_samples=N, noise=0.1)\n",
    "y= one_hot_coding(y)\n",
    "print(X.shape)\n",
    "\n",
    "scatter(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    accuracy 구하기\n",
    "\n",
    "    Args:\n",
    "        y_true: true label\n",
    "        y_hat: predicted label\n",
    "    Returns:\n",
    "        Accuracy\n",
    "    Hints\n",
    "        - tf.equal : 값을 비교하여 동일하면 1, 다르면 0을 return\n",
    "        - tf.argmax : 값이 최대인 위치의 index를 return\n",
    "        - tf.cast : data type을 변경하는 함수\n",
    "        - tf.reduce_mean : 지정한 축(axis)에 대해 평균을 취하는 함수.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('accuracy'):\n",
    "        prediction = tf.argmax(y_pred, axis=1)\n",
    "        true = tf.argmax(y_true, axis=1)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(true, prediction), 'float'))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier (선형 분류기)\n",
    "\n",
    "## <center> \\\\( f(x) = xW+b \\\\)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_classifier(x, out_dim, name):\n",
    "    \"\"\"\n",
    "    Args :\n",
    "        Inputs : Input tensor\n",
    "        out_dim : output dimension\n",
    "        name : name for parameter\n",
    "    Returns:\n",
    "        output\n",
    "        [weights] : list\n",
    "        [biases] : list\n",
    "    Hints\n",
    "        - tf.Variable  \n",
    "        - tf.matmul\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        in_dim = x.get_shape().as_list()[1]\n",
    "        w = tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=0.01), tf.float32)\n",
    "        b = tf.Variable(tf.zeros(out_dim), tf.float32)\n",
    "        output = tf.add(tf.matmul(x, w),b)\n",
    "    return output, [w], [b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function (손실 함수) : Cross Entropy\n",
    "\n",
    "# <center> \\\\( L(y_i, f(x_i; W)) = -\\frac{1}{n}\\sum_{i=1}^{n} y_{i} log(f(x_i))\\\\)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_true, y_pred, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    compute cross entropy\n",
    "\n",
    "    Args:\n",
    "        y_true: true label\n",
    "        y_hat: predicted label\n",
    "        epsilon: small value to prevent NaN in log\n",
    "\n",
    "    Returns:\n",
    "        cross entropy loss\n",
    "    Hints\n",
    "        - tf.reduce_mean  \n",
    "        - tf.reduce_sum\n",
    "        - tf.log\n",
    "    \"\"\"\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        loss = -tf.reduce_mean(tf.reduce_sum(y_true*tf.log(y_pred+epsilon), axis=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. With Softmax Linear Classifier\n",
    "## <center> \\\\( f(x_i; W,b) =  \\sigma(xW+b) \\\\)</center>\n",
    "\n",
    "> where $\\sigma$ is softmax function, i.e.,\n",
    "\n",
    "## <center> \\\\( \\sigma(x)_i =  \\frac{e^{x_i}}{\\sum_j e^{x_j}}  \\\\)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "iteration = 100\n",
    "step_size = iteration/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(x, k):\n",
    "    \"\"\"\n",
    "        Softmax Linear Classifier Model\n",
    "        \n",
    "        Args:\n",
    "            x : input\n",
    "            k : the number of classes, i.e., output dimension of the model\n",
    "        Returns:\n",
    "            output\n",
    "            weights list\n",
    "            bias list\n",
    "        Hints:\n",
    "            tf.nn.softmax : softmax function\n",
    "    \"\"\"\n",
    "    out, w, b = linear_classifier(x, k, 'fc1')\n",
    "    out = tf.nn.softmax(out)\n",
    "    return out, w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 1) Declare Placeholders\n",
    "# input, true_label을 위한 placeholder 선언\n",
    "# Hint : tf.placeholder, input과 ouput이 어떤 차원을 가져야할지 고민해봅시다.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "x = tf.placeholder(tf.float32, [None, 2], 'x')\n",
    "y_true = tf.placeholder(tf.float32, [None, K], 'y')\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 2) model declare\n",
    "# Prediction을 위해 기존의 정의한 model을 사용합니다.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "y_pred, w, b = model_1(x, K)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 3) obtain loss & accuracy\n",
    "# 앞서 정의한 function들을 통해 loss와 accuracy를 얻습니다.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "loss = cross_entropy(y_true, y_pred)\n",
    "acc = accuracy(y_true, y_pred)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 4) Optimizer\n",
    "# Parameter Update를 위한 Optimizer를 선언합니다. Learning rate는 learning_rate로 선언된 변수를 사용합니다.\n",
    "# Hint : tf.train.GradientDescentOptimizer\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "train_ops = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 7) Session \n",
    "# tensorflow의 연산을 수행하기 위해 session을 호출합니다.\n",
    "# 8) variable iniializer \n",
    "# tensorflow의 variable들을 초기화 하기 위해 initializer를 사용합니다.\n",
    "# Hint : tf.global_variables_initializer\n",
    "# 9) training \n",
    "# 학습에서 사용할 iteration 수는 위의 iteration을 사용합니다. Loss 및 accuracy를 check할 때 step_size를 사용합니다.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "j = 0\n",
    "with tf.Session() as sess:\n",
    "    train_x, train_y = X,y\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(iteration):\n",
    "        _,loss_ = sess.run([train_ops, loss], feed_dict={x:train_x, y_true:train_y})  \n",
    "        if (i+1)%step_size == 0:\n",
    "            pred, acc_, weights, biases = sess.run([y_pred, acc, w, b], feed_dict={x:train_x, y_true:train_y})\n",
    "            pred = np.argmax(pred, axis=1)\n",
    "            print(\"{} iteration, loss : {:.4f}, accuracy : {:.4f}\".format(i+1, loss_, acc_))\n",
    "            scatter(train_x, pred)\n",
    "            shape_scatter(train_x, pred, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. With Neural Network (Fully Connected Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2(x, k):\n",
    "    \"\"\"\n",
    "        Neural Network with 2 Hidden layer.\n",
    "        Args:\n",
    "            x : input\n",
    "            k : the number of classes, i.e., output dimension of the model\n",
    "        Returns:\n",
    "            output\n",
    "            weights list\n",
    "            bias list\n",
    "        Hint:\n",
    "            tf.nn.relu : activation function\n",
    "            tf.nn.softmax : softmax function\n",
    "    \"\"\"\n",
    "    \n",
    "    out, w1, b1 = linear_classifier(x, 100, 'fc1')\n",
    "    out = tf.nn.relu(out)\n",
    "    out, w2, b2= linear_classifier(out, 100, 'fc2')\n",
    "    out = tf.nn.relu(out)\n",
    "    out, w3, b3 = linear_classifier(out, k, 'fc3')\n",
    "    out = tf.nn.softmax(out)\n",
    "    return out, w1+w2+w3, b1+b2+b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=200\n",
    "K=2\n",
    "learning_rate = 1e-0\n",
    "iteration = 240\n",
    "l = 0\n",
    "step_size = iteration/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, [None, 2], 'x')\n",
    "y_true = tf.placeholder(tf.float32, [None, K], 'y')\n",
    "\n",
    "y_pred, w, b = model_2(x, K)\n",
    "loss = cross_entropy(y_true, y_pred)\n",
    "acc = accuracy(y_true, y_pred)\n",
    "\n",
    "train_ops = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "j=1\n",
    "with tf.Session() as sess:\n",
    "    train_x, train_y = X,y\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(iteration):\n",
    "        _,loss_ = sess.run([train_ops, loss], feed_dict={x:train_x, y_true:train_y})  \n",
    "        if (i+1)%2 == 0:\n",
    "            pred, acc_, weights, biases = sess.run([y_pred, acc, w, b], feed_dict={x:train_x, y_true:train_y})\n",
    "            pred = np.argmax(pred, axis=1)\n",
    "            shape_scatter(train_x, pred, weights, biases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. With Your work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_3(x, k):\n",
    "    \"\"\"\n",
    "        Neural Network with 2 Hidden layer.\n",
    "        Args:\n",
    "            x : input\n",
    "            k : the number of classes, i.e., output dimension of the model\n",
    "        Returns:\n",
    "            output\n",
    "            weights list\n",
    "            bias list\n",
    "        Hint:\n",
    "            tf.nn.relu : activation function\n",
    "            tf.nn.softmax : softmax function\n",
    "    \"\"\"\n",
    "    \n",
    "    out, w1, b1 = linear_classifier(x, 100, 'fc1')\n",
    "    out = tf.nn.relu(out)\n",
    "    out, w2, b2= linear_classifier(out, 100, 'fc2')\n",
    "    out = tf.nn.relu(out)\n",
    "    out, w3, b3= linear_classifier(out, 100, 'fc3')\n",
    "    out = tf.nn.relu(out)\n",
    "    out, w4, b4 = linear_classifier(out, k, 'fc4')\n",
    "    out = tf.nn.softmax(out)\n",
    "    return out, w1+w2+w3+w4, b1+b2+b3+b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-0\n",
    "iteration = 1000\n",
    "l = 3e-4\n",
    "step_size = iteration/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, [None, 2], 'x')\n",
    "y_true = tf.placeholder(tf.float32, [None, K], 'y')\n",
    "\n",
    "y_pred, w, b = model_3(x, K)\n",
    "loss = cross_entropy(y_true, y_pred)\n",
    "acc = accuracy(y_true, y_pred)\n",
    "\n",
    "train_ops = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_x, train_y = spiral_data(n=N, k=K)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(iteration):\n",
    "        _,loss_ = sess.run([train_ops, loss], feed_dict={x:train_x, y_true:train_y})  \n",
    "        if (i+1)%100 == 0:\n",
    "            clear_output()\n",
    "            pred, acc_, weights, biases = sess.run([y_pred, acc, w, b], feed_dict={x:train_x, y_true:train_y})\n",
    "            pred = np.argmax(pred, axis=1)\n",
    "            print(\"{} iteration, loss : {:.4f}, accuracy : {:.4f}\".format(i+1, loss_, acc_))\n",
    "            shape_scatter(train_x, pred, weights, biases)\n",
    "            time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
