{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 데이터 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Downloading mnist.pkl.gz 100.0 %()\n",
      "('Successfully donloaded', 'mnist.pkl.gz', 16168813, 'bytes.')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from six.moves import urllib\n",
    "import gzip\n",
    "import cPickle\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "SOURCE_URL = 'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "FILENAME = SOURCE_URL.split('/')[-1]\n",
    "DATA_DIR = '../datasets'\n",
    "\n",
    "def maybe_download(data_dir):\n",
    "    filepath = os.path.join(data_dir, FILENAME)\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    if not os.path.isfile(filepath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\r>> Downloading {} {:.1f} %'.format(\n",
    "                FILENAME, float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "        filepath, _ = urllib.request.urlretrieve(SOURCE_URL, filepath, _progress)\n",
    "        print()\n",
    "        statinfo = os.stat(filepath)\n",
    "        print('Successfully donloaded', FILENAME, statinfo.st_size, 'bytes.')\n",
    "\n",
    "def load(data_dir, subset='train'):\n",
    "    maybe_download(data_dir)\n",
    "    filepath = os.path.join(data_dir, FILENAME)\n",
    "    \n",
    "    f = gzip.open(filepath, 'rb')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    if subset == 'train':\n",
    "        trainx, trainy = train_set\n",
    "        trainx = trainx.astype(np.float32).reshape(trainx.shape[0], 28, 28)\n",
    "        trainy = trainy.astype(np.uint8)\n",
    "        return trainx, trainy\n",
    "    elif subset == 'test':\n",
    "        testx, testy = test_set\n",
    "        testx = testx.astype(np.float32).reshape(testx.shape[0], 28, 28)\n",
    "        testy = testy.astype(np.uint8)\n",
    "        return testx, testy\n",
    "    elif subset== 'valid':\n",
    "        validx, validy = valid_set\n",
    "        validx = validx.astype(np.float32).reshape(validx.shape[0], 28, 28)\n",
    "        validy = validy.astype(np.uint8)\n",
    "        return validx, validy\n",
    "    else:\n",
    "        raise NotImplementedError('subset should be train or valid or test')\n",
    "\n",
    "# Load data\n",
    "train_data, train_label = load(DATA_DIR, 'train')\n",
    "valid_data, valid_label = load(DATA_DIR, 'valid')\n",
    "test_data, test_label = load(DATA_DIR, 'test')\n",
    "\n",
    "# concatenate train and valid data as train data\n",
    "train_data = np.concatenate((train_data, valid_data))\n",
    "train_label = np.concatenate((train_label, valid_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 데이터 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# size of MNIST\n",
    "print train_data.shape\n",
    "print train_label.shape\n",
    "print test_data.shape\n",
    "print test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f22316b8290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show data\n",
    "_, (ax1, ax2) = plt.subplots(1, 2)\n",
    "sample_data = train_data[0]\n",
    "ax1.imshow(sample_data, cmap=plt.cm.Greys);\n",
    "ax2.hist(sample_data, bins=20, range=[0, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesImage(54,36;334.8x217.44)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f224c106090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(plt.imshow(sample_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing (데이터 전처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "SUMMARY_DIR = './summary'\n",
    "TRAIN_DIR = SUMMARY_DIR + '/train'\n",
    "TEST_DIR = SUMMARY_DIR + '/test'\n",
    "\n",
    "if os.path.exists(SUMMARY_DIR):\n",
    "    shutil.rmtree(SUMMARY_DIR)\n",
    "if not os.path.exists(SUMMARY_DIR):\n",
    "    os.makedirs(SUMMARY_DIR)\n",
    "    os.makedirs(TRAIN_DIR)\n",
    "    os.makedirs(TEST_DIR)\n",
    "\n",
    "def get_next_batch(data, label, batch_size):\n",
    "    \"\"\"\n",
    "    get 'batch_size' amount of data and label randomly\n",
    "\n",
    "    Args:\n",
    "        data: data\n",
    "        label: label\n",
    "        batch_size: # of data to get\n",
    "\n",
    "    Returns:\n",
    "        batch_data: data of 'batch_size'\n",
    "        batch_label: coresponding label of batch_data\n",
    "    \"\"\"\n",
    "    n_data = data.shape[0]\n",
    "    random_idx = random.sample(range(1, n_data), batch_size)\n",
    "    \n",
    "    batch_data = data[random_idx]\n",
    "    batch_label = label[random_idx]\n",
    "    return batch_data, batch_label\n",
    "\n",
    "def get_one_hot(label):\n",
    "    \"\"\"\n",
    "    get one hot encoded label matrix\n",
    "\n",
    "    Args:\n",
    "        label: original label\n",
    "\n",
    "    Returns:\n",
    "        One hot encoded label matrix\n",
    "    \"\"\"\n",
    "    # Make 0 initialized numpy array with shape of [label.shape[0], 10]\n",
    "    one_hot = np.zeros((label.shape[0], 10))\n",
    "    # Fill up the array according to the input label\n",
    "    one_hot[np.arange(label.shape[0]), label.astype(int)] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation (1)\n",
    "\n",
    "\n",
    "## Loss function (손실 함수) : Cross Entropy\n",
    "\n",
    "# <center> \\\\( L(y_i, f(x_i; W)) = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{k=1} y_{i,j} log(f(x_i)_k)\\\\)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cross_entropy_loss(y_true, y_hat, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    compute cross entropy\n",
    "\n",
    "    Args:\n",
    "        y_true: true label\n",
    "        y_hat: predicted label\n",
    "        epsilon: small value to prevent NaN in log\n",
    "\n",
    "    Returns:\n",
    "        cross entropy loss\n",
    "    Hints\n",
    "        - tf.reduce_mean  \n",
    "        - tf.reduce_sum\n",
    "        - tf.log\n",
    "    \"\"\"\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        cross_entropy = \n",
    "    return cross_entropy\n",
    "\n",
    "def get_accuracy(y_true, y_hat):\n",
    "    \"\"\"\n",
    "    accuracy 구하기\n",
    "\n",
    "    Args:\n",
    "        y_true: true label\n",
    "        y_hat: predicted label\n",
    "    Returns:\n",
    "        Accuracy\n",
    "    Hints\n",
    "        - tf.equal : 값을 비교하여 동일하면 1, 다르면 0을 return\n",
    "        - tf.argmax : 값이 최대인 위치의 index를 return\n",
    "        - tf.cast : data type을 변경하는 함수\n",
    "        - tf.reduce_mean : 지정한 축(axis)에 대해 평균을 취하는 함수.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('accuracy'):\n",
    "        # Compare the highest indices between the predicted label and the true label\n",
    "        correct_prediction = \n",
    "        # Compute accuracy\n",
    "        accuracy = \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter (하이퍼 파라미터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set hypyerparameters\n",
    "learning_rate = 0.01\n",
    "iteration = 2000\n",
    "step_size = iteration/10\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation (2)\n",
    "## Linear Classifier (선형 분류기)\n",
    "\n",
    "## <center> \\\\( f(x) = xW+b \\\\)</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(inputs, out_dim, name):\n",
    "    \"\"\"\n",
    "    Args :\n",
    "        Inputs : Input tensor\n",
    "        out_dim : output dimension\n",
    "        name : name for parameter\n",
    "    Returns:\n",
    "        output\n",
    "    Hints\n",
    "        - tf.Variable  \n",
    "        - tf.matmul\n",
    "    \"\"\"\n",
    "    with tf.name_scope('linear_classifier'):\n",
    "        # Get dimension of inputs (not batch)\n",
    "        in_dim = \n",
    "        # initializer for weight matrix -> tf.zeros([])\n",
    "        W = \n",
    "        # initializer for bias vector -> tf.zeros([])\n",
    "        b = \n",
    "        # compute logits\n",
    "        y_logits =\n",
    "    return y_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. With Softmax Linear Classifier\n",
    "## <center> \\\\( f(x_i; W,b) =  \\sigma(xW+b) \\\\)</center>\n",
    "\n",
    "> where $\\sigma$ is softmax function, i.e.,\n",
    "\n",
    "## <center> \\\\( \\sigma(x)_i =  \\frac{e^{x_i}}{\\sum_j e^{x_j}}  \\\\)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(x):\n",
    "    \"\"\"\n",
    "        Softmax Linear Classifier Model\n",
    "        \n",
    "        Args:\n",
    "            x : input\n",
    "            k : the number of classes, i.e., output dimension of the model\n",
    "        Returns:\n",
    "            output\n",
    "            weights list\n",
    "            bias list\n",
    "        Hints:\n",
    "            tf.nn.softmax : softmax function\n",
    "    \"\"\"\n",
    "    out = \n",
    "    out = \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "train_label_hot = get_one_hot(train_label)\n",
    "test_label_hot = get_one_hot(test_label)\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 0) Data_flatten\n",
    "# 3차원 Tensor를 2차원 maxtrix로 reshape\n",
    "# Hint : np.reshape()\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "train_data =\n",
    "test_data = \n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 1) Declare Placeholders\n",
    "# input, true_label을 위한 placeholder 선언\n",
    "# Hint : tf.placeholder, input과 ouput이 어떤 차원을 가져야할지 고민해봅시다.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "x = \n",
    "y_true =\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 2) model declare\n",
    "# Prediction을 위해 기존의 정의한 model을 사용합니다.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "y_pred = \n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 3) obtain loss & accuracy\n",
    "# 앞서 정의한 function들을 통해 loss와 accuracy를 얻습니다.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "cross_entropy = \n",
    "accuracy = \n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 4) Optimizer\n",
    "# Parameter Update를 위한 Optimizer를 선언합니다. Learning rate는 learning_rate로 선언된 변수를 사용합니다.\n",
    "# Hint : tf.train.GradientDescentOptimizer\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 7) Session \n",
    "# tensorflow의 연산을 수행하기 위해 session을 호출합니다.\n",
    "# 8) variable iniializer \n",
    "# tensorflow의 variable들을 초기화 하기 위해 initializer를 사용합니다.\n",
    "# Hint : tf.global_variables_initializer\n",
    "# 9) training \n",
    "# 학습에서 사용할 iteration 수는 위의 iteration을 사용합니다. Loss 및 accuracy를 check할 때 step_size를 사용합니다.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "  \n",
    "    # Training\n",
    "    for step in range(max_iter):\n",
    "        # Get batch data and label\n",
    "        batch_x, batch_y = get_next_batch(train_data, train_label_hot, batch_size)\n",
    "        # train the network and calculate cross entropy\n",
    "        _, loss = sess.run([train_step, cross_entropy], feed_dict={x: batch_x, y_true: batch_y})\n",
    "        # calcualte accuracy\n",
    "        acc = sess.run(accuracy, feed_dict={x: test_data, y_true: test_label_hot})\n",
    "        # print loss (cross entropy) and accuracy at every 10th step\n",
    "        if (step + 1) % step_size == 0:\n",
    "            print(\"{}th iteration, loss: {:.4f}, test accuracy: {:.4f}\".format(step + 1, loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation (4)\n",
    "\n",
    "## Non-Linear Classifier & Multi-Layer Perceptron\n",
    "\n",
    "## <center> \\\\( f(x) = W^T_2\\sigma(W^T_1x+b_1)+b_2 \\\\)</center>\n",
    "\n",
    "### 위에서 완성한 linear function과 relu activation function을 활용하여, 1개의 hidden layer가 있는 MLP를 구현하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_2(Inputs, hidden, out_dim):\n",
    "    \"\"\"\n",
    "        Neural Network with 1 Hidden layer.\n",
    "        Args:\n",
    "            x : input\n",
    "            k : the number of classes, i.e., output dimension of the model\n",
    "        Returns:\n",
    "            output\n",
    "            weights list\n",
    "            bias list\n",
    "        Hint:\n",
    "            tf.nn.relu : activation function\n",
    "            tf.nn.softmax : softmax function\n",
    "    \"\"\"\n",
    "    # First fully connected layer\n",
    "    h = \n",
    "    # add non-linearity (using tf.nn.relu(input))\n",
    "    h_relu =\n",
    "    # Second fully connected layer\n",
    "    y_logits = \n",
    "    out =\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation (5)\n",
    "\n",
    "## Model Setting\n",
    "\n",
    "### 1. Training data 및 Test data의 각각의 image를 한 vector로 만들어서 train_data, test_data에 각각 저장하세요.\n",
    "#### Hint) 데이터 차원.\n",
    "### 2. Dataset로부터 받은 데이터(Image, label)를 담을 변수를 각각 x 및 y_true에 선언하고, DropOut을 위한 확률 값을 담을 변수를 keep_prob에 저장하세요.\n",
    "#### Hint) tf.placeholder\n",
    "### 3. Implementation (4)에서 구현한  Multilayer perceptron 함수와 softmax 함수를 통한 prediction 값을 y_hat에 저장하세요.\n",
    "#### Hint) tf.nn.softmax\n",
    "### 4. 3으로부터 얻은 결과를 통해 Implementation (1)에서 구현한 loss function을 통해 얻은 loss를 cross_entropy에 저장하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "train_label_hot = get_one_hot(train_label)\n",
    "test_label_hot = get_one_hot(test_label)\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 0) Data_flatten\n",
    "# 3차원 Tensor를 2차원 maxtrix로 reshape\n",
    "# Hint : np.reshape()\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "train_data = \n",
    "test_data = \n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 1) Declare Placeholders\n",
    "# input, true_label을 위한 placeholder 선언\n",
    "# Hint : tf.placeholder, input과 ouput이 어떤 차원을 가져야할지 고민해봅시다.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "x = \n",
    "y_true = \n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 2) model declare\n",
    "# Prediction을 위해 기존의 정의한 model을 사용합니다.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "y_pred =\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 3) obtain loss & accuracy\n",
    "# 앞서 정의한 function들을 통해 loss와 accuracy를 얻습니다.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "cross_entropy = \n",
    "accuracy = \n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 4) Optimizer\n",
    "# Parameter Update를 위한 Optimizer를 선언합니다. Learning rate는 learning_rate로 선언된 변수를 사용합니다.\n",
    "# Hint : tf.train.GradientDescentOptimizer\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# 7) Session \n",
    "# tensorflow의 연산을 수행하기 위해 session을 호출합니다.\n",
    "# 8) variable iniializer \n",
    "# tensorflow의 variable들을 초기화 하기 위해 initializer를 사용합니다.\n",
    "# Hint : tf.global_variables_initializer\n",
    "# 9) training \n",
    "# 학습에서 사용할 iteration 수는 위의 iteration을 사용합니다. Loss 및 accuracy를 check할 때 step_size를 사용합니다.\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "  \n",
    "    # Training\n",
    "    for step in range(max_iter):\n",
    "        # Get batch data and label\n",
    "        batch_x, batch_y = get_next_batch(train_data, train_label_hot, batch_size)\n",
    "        # train the network and calculate cross entropy\n",
    "        _, loss = sess.run([train_step, cross_entropy], feed_dict={x: batch_x, y_true: batch_y})\n",
    "        # calcualte accuracy\n",
    "        acc = sess.run(accuracy, feed_dict={x: test_data, y_true: test_label_hot})\n",
    "        # print loss (cross entropy) and accuracy at every 10th step\n",
    "        if (step + 1) % step_size == 0:\n",
    "            print(\"{}th iteration, loss: {:.4f}, test accuracy: {:.4f}\".format(step + 1, loss, acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
